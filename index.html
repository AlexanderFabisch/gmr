<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>gmr API documentation</title>
<meta name="description" content="gmr
=== …">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>gmr</code></h1>
</header>
<section id="section-intro">
<h1 id="gmr">gmr</h1>
<p>Gaussian Mixture Models (GMMs) for clustering and regression in Python.</p>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="gmr.gmm" href="gmm.html">gmr.gmm</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="gmr.mvn" href="mvn.html">gmr.mvn</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="gmr.sklearn" href="sklearn.html">gmr.sklearn</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="gmr.tests" href="tests/index.html">gmr.tests</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="gmr.utils" href="utils.html">gmr.utils</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="gmr.covariance_initialization"><code class="name flex">
<span>def <span class="ident">covariance_initialization</span></span>(<span>X, n_components)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def covariance_initialization(X, n_components):
    &#34;&#34;&#34;Initialize covariances.

    The standard deviation in each dimension is set to the average Euclidean
    distance of the training samples divided by the number of components.

    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        Samples from the true distribution.

    n_components : int (&gt; 0)
        Number of MVNs that compose the GMM.

    Returns
    -------
    initial_covariances : array, shape (n_components, n_features, n_features)
        Initial covariances
    &#34;&#34;&#34;
    if n_components &lt;= 0:
        raise ValueError(
            &#34;It does not make sense to initialize 0 or fewer covariances.&#34;)
    n_features = X.shape[1]
    average_distances = np.empty(n_features)
    for i in range(n_features):
        average_distances[i] = np.mean(
            pdist(X[:, i, np.newaxis], metric=&#34;euclidean&#34;))
    initial_covariances = np.empty((n_components, n_features, n_features))
    initial_covariances[:] = (np.eye(n_features) *
                              (average_distances / n_components) ** 2)
    return initial_covariances</code></pre>
</details>
<div class="desc"><p>Initialize covariances.</p>
<p>The standard deviation in each dimension is set to the average Euclidean
distance of the training samples divided by the number of components.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like, shape (n_samples, n_features)</code></dt>
<dd>Samples from the true distribution.</dd>
<dt><strong><code>n_components</code></strong> :&ensp;<code>int (&gt; 0)</code></dt>
<dd>Number of MVNs that compose the GMM.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>initial_covariances</code></strong> :&ensp;<code>array, shape (n_components, n_features, n_features)</code></dt>
<dd>Initial covariances</dd>
</dl></div>
</dd>
<dt id="gmr.kmeansplusplus_initialization"><code class="name flex">
<span>def <span class="ident">kmeansplusplus_initialization</span></span>(<span>X, n_components, random_state=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kmeansplusplus_initialization(X, n_components, random_state=None):
    &#34;&#34;&#34;k-means++ initialization for centers of a GMM.

    Initialization of GMM centers before expectation maximization (EM).
    The first center is selected uniformly random. Subsequent centers are
    sampled from the data with probability proportional to the squared
    distance to the closest center.

    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        Samples from the true distribution.

    n_components : int (&gt; 0)
        Number of MVNs that compose the GMM.

    random_state : int or RandomState, optional (default: global random state)
        If an integer is given, it fixes the seed. Defaults to the global numpy
        random number generator.

    Returns
    -------
    initial_means : array, shape (n_components, n_features)
        Initial means
    &#34;&#34;&#34;
    if n_components &lt;= 0:
        raise ValueError(&#34;Only n_components &gt; 0 allowed.&#34;)
    if n_components &gt; len(X):
        raise ValueError(
            &#34;More components (%d) than samples (%d) are not allowed.&#34;
            % (n_components, len(X)))

    random_state = check_random_state(random_state)

    all_indices = np.arange(len(X))
    selected_centers = [random_state.choice(all_indices, size=1).tolist()[0]]
    while len(selected_centers) &lt; n_components:
        centers = np.atleast_2d(X[np.array(selected_centers, dtype=int)])
        i = _select_next_center(X, centers, random_state, selected_centers, all_indices)
        selected_centers.append(i)
    return X[np.array(selected_centers, dtype=int)]</code></pre>
</details>
<div class="desc"><p>k-means++ initialization for centers of a GMM.</p>
<p>Initialization of GMM centers before expectation maximization (EM).
The first center is selected uniformly random. Subsequent centers are
sampled from the data with probability proportional to the squared
distance to the closest center.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like, shape (n_samples, n_features)</code></dt>
<dd>Samples from the true distribution.</dd>
<dt><strong><code>n_components</code></strong> :&ensp;<code>int (&gt; 0)</code></dt>
<dd>Number of MVNs that compose the GMM.</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code> or <code>RandomState</code>, optional <code>(default: global random state)</code></dt>
<dd>If an integer is given, it fixes the seed. Defaults to the global numpy
random number generator.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>initial_means</code></strong> :&ensp;<code>array, shape (n_components, n_features)</code></dt>
<dd>Initial means</dd>
</dl></div>
</dd>
<dt id="gmr.plot_error_ellipse"><code class="name flex">
<span>def <span class="ident">plot_error_ellipse</span></span>(<span>ax,<br>mvn,<br>color=None,<br>alpha=0.25,<br>factors=array([0.25, 0.5 , 0.75, 1.
, 1.25, 1.5 , 1.75, 2.
]))</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_error_ellipse(ax, mvn, color=None, alpha=0.25,
                       factors=np.linspace(0.25, 2.0, 8)):
    &#34;&#34;&#34;Plot error ellipse of MVN.

    Parameters
    ----------
    ax : axis
        Matplotlib axis.

    mvn : MVN
        Multivariate normal distribution.

    color : str, optional (default: None)
        Color in which the ellipse should be plotted

    alpha : float, optional (default: 0.25)
        Alpha value for ellipse

    factors : array, optional (default: np.linspace(0.25, 2.0, 8))
        Multiples of the standard deviations that should be plotted.
    &#34;&#34;&#34;
    from matplotlib.patches import Ellipse
    for factor in factors:
        angle, width, height = mvn.to_ellipse(factor)
        ell = Ellipse(xy=mvn.mean, width=2.0 * width, height=2.0 * height,
                      angle=np.degrees(angle))
        ell.set_alpha(alpha)
        if color is not None:
            ell.set_color(color)
        ax.add_artist(ell)</code></pre>
</details>
<div class="desc"><p>Plot error ellipse of MVN.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ax</code></strong> :&ensp;<code>axis</code></dt>
<dd>Matplotlib axis.</dd>
<dt><strong><code>mvn</code></strong> :&ensp;<code><a title="gmr.MVN" href="#gmr.MVN">MVN</a></code></dt>
<dd>Multivariate normal distribution.</dd>
<dt><strong><code>color</code></strong> :&ensp;<code>str</code>, optional <code>(default: None)</code></dt>
<dd>Color in which the ellipse should be plotted</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code>, optional <code>(default: 0.25)</code></dt>
<dd>Alpha value for ellipse</dd>
<dt><strong><code>factors</code></strong> :&ensp;<code>array</code>, optional <code>(default: np.linspace(0.25, 2.0, 8))</code></dt>
<dd>Multiples of the standard deviations that should be plotted.</dd>
</dl></div>
</dd>
<dt id="gmr.plot_error_ellipses"><code class="name flex">
<span>def <span class="ident">plot_error_ellipses</span></span>(<span>ax,<br>gmm,<br>colors=None,<br>alpha=0.25,<br>factors=array([0.25, 0.5 , 0.75, 1.
, 1.25, 1.5 , 1.75, 2.
]))</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_error_ellipses(ax, gmm, colors=None, alpha=0.25, factors=np.linspace(0.25, 2.0, 8)):
    &#34;&#34;&#34;Plot error ellipses of GMM components.

    Parameters
    ----------
    ax : axis
        Matplotlib axis.

    gmm : GMM
        Gaussian mixture model.

    colors : list of str, optional (default: None)
        Colors in which the ellipses should be plotted

    alpha : float, optional (default: 0.25)
        Alpha value for ellipses

    factors : array, optional (default: np.linspace(0.25, 2.0, 8))
        Multiples of the standard deviations that should be plotted.
    &#34;&#34;&#34;
    from matplotlib.patches import Ellipse
    from itertools import cycle
    if colors is not None:
        colors = cycle(colors)
    for factor in factors:
        for mean, (angle, width, height) in gmm.to_ellipses(factor):
            ell = Ellipse(xy=mean, width=2.0 * width, height=2.0 * height,
                          angle=np.degrees(angle))
            ell.set_alpha(alpha)
            if colors is not None:
                ell.set_color(next(colors))
            ax.add_artist(ell)</code></pre>
</details>
<div class="desc"><p>Plot error ellipses of GMM components.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ax</code></strong> :&ensp;<code>axis</code></dt>
<dd>Matplotlib axis.</dd>
<dt><strong><code>gmm</code></strong> :&ensp;<code><a title="gmr.GMM" href="#gmr.GMM">GMM</a></code></dt>
<dd>Gaussian mixture model.</dd>
<dt><strong><code>colors</code></strong> :&ensp;<code>list</code> of <code>str</code>, optional <code>(default: None)</code></dt>
<dd>Colors in which the ellipses should be plotted</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code>, optional <code>(default: 0.25)</code></dt>
<dd>Alpha value for ellipses</dd>
<dt><strong><code>factors</code></strong> :&ensp;<code>array</code>, optional <code>(default: np.linspace(0.25, 2.0, 8))</code></dt>
<dd>Multiples of the standard deviations that should be plotted.</dd>
</dl></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="gmr.GMM"><code class="flex name class">
<span>class <span class="ident">GMM</span></span>
<span>(</span><span>n_components, priors=None, means=None, covariances=None, verbose=0, random_state=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GMM(object):
    &#34;&#34;&#34;Gaussian Mixture Model.

    Parameters
    ----------
    n_components : int
        Number of MVNs that compose the GMM.

    priors : array-like, shape (n_components,), optional
        Weights of the components.

    means : array-like, shape (n_components, n_features), optional
        Means of the components.

    covariances : array-like, shape (n_components, n_features, n_features), optional
        Covariances of the components.

    verbose : int, optional (default: 0)
        Verbosity level.

    random_state : int or RandomState, optional (default: global random state)
        If an integer is given, it fixes the seed. Defaults to the global numpy
        random number generator.
    &#34;&#34;&#34;
    def __init__(self, n_components, priors=None, means=None, covariances=None,
                 verbose=0, random_state=None):
        self.n_components = n_components
        self.priors = priors
        self.means = means
        self.covariances = covariances
        self.verbose = verbose
        self.random_state = check_random_state(random_state)

        if self.priors is not None:
            self.priors = np.asarray(self.priors)
        if self.means is not None:
            self.means = np.asarray(self.means)
        if self.covariances is not None:
            self.covariances = np.asarray(self.covariances)

    def _check_initialized(self):
        if self.priors is None:
            raise ValueError(&#34;Priors have not been initialized&#34;)
        if self.means is None:
            raise ValueError(&#34;Means have not been initialized&#34;)
        if self.covariances is None:
            raise ValueError(&#34;Covariances have not been initialized&#34;)

    def from_samples(self, X, R_diff=1e-4, n_iter=100, init_params=&#34;random&#34;,
                     oracle_approximating_shrinkage=False):
        &#34;&#34;&#34;MLE of the mean and covariance.

        Expectation-maximization is used to infer the model parameters. The
        objective function is non-convex. Hence, multiple runs can have
        different results.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Samples from the true distribution.

        R_diff : float
            Minimum allowed difference of responsibilities between successive
            EM iterations.

        n_iter : int
            Maximum number of iterations.

        init_params : str, optional (default: &#39;random&#39;)
            Parameter initialization strategy. If means and covariances are
            given in the constructor, this parameter will have no effect.
            &#39;random&#39; will sample initial means randomly from the dataset
            and set covariances to identity matrices. This is the
            computationally cheap solution.
            &#39;kmeans++&#39; will use k-means++ initialization for means and
            initialize covariances to diagonal matrices with variances
            set based on the average distances of samples in each dimensions.
            This is computationally more expensive but often gives much
            better results.

        oracle_approximating_shrinkage : bool, optional (default: False)
            Use Oracle Approximating Shrinkage (OAS) estimator for covariances
            to ensure positive semi-definiteness.

        Returns
        -------
        self : GMM
            This object.
        &#34;&#34;&#34;
        n_samples, n_features = X.shape

        if self.priors is None:
            self.priors = np.ones(self.n_components,
                                  dtype=float) / self.n_components

        if init_params not in [&#34;random&#34;, &#34;kmeans++&#34;]:
            raise ValueError(&#34;&#39;init_params&#39; must be &#39;random&#39; or &#39;kmeans++&#39; &#34;
                             &#34;but is &#39;%s&#39;&#34; % init_params)

        if self.means is None:
            if init_params == &#34;random&#34;:
                indices = self.random_state.choice(
                    np.arange(n_samples), self.n_components)
                self.means = X[indices]
            else:
                self.means = kmeansplusplus_initialization(
                    X, self.n_components, self.random_state)

        if self.covariances is None:
            if init_params == &#34;random&#34;:
                self.covariances = np.empty(
                    (self.n_components, n_features, n_features))
                self.covariances[:] = np.eye(n_features)
            else:
                self.covariances = covariance_initialization(
                    X, self.n_components)

        R = np.zeros((n_samples, self.n_components))
        for _ in range(n_iter):
            R_prev = R

            # Expectation
            R = self.to_responsibilities(X)

            if np.linalg.norm(R - R_prev) &lt; R_diff:
                if self.verbose:
                    print(&#34;EM converged.&#34;)
                break

            # Maximization
            w = R.sum(axis=0) + 10.0 * np.finfo(R.dtype).eps
            R_n = R / w
            self.priors = w / w.sum()
            self.means = R_n.T.dot(X)
            for k in range(self.n_components):
                Xm = X - self.means[k]
                self.covariances[k] = (R_n[:, k, np.newaxis] * Xm).T.dot(Xm)

            if oracle_approximating_shrinkage:
                n_samples_eff = (np.sum(R_n, axis=0) ** 2 /
                                 np.sum(R_n ** 2, axis=0))
                self.apply_oracle_approximating_shrinkage(n_samples_eff)

        return self

    def apply_oracle_approximating_shrinkage(self, n_samples_eff):
        &#34;&#34;&#34;Apply Oracle Approximating Shrinkage to covariances.

        Empirical covariances might have negative eigenvalues caused by
        numerical issues so that they are not positive semi-definite and
        are not invertible. In these cases it is better to apply this
        function after training. You can also apply it after each
        EM step with a flag of `GMM.from_samples`.

        This is an implementation of

        Chen et al., “Shrinkage Algorithms for MMSE Covariance Estimation”,
        IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.

        based on the implementation of scikit-learn:

        https://scikit-learn.org/stable/modules/generated/oas-function.html#sklearn.covariance.oas

        Parameters
        ----------
        n_samples_eff : float or array-like, shape (n_components,)
            Number of effective samples from which the covariances have been
            estimated. A rough estimate would be the size of the dataset.
            Covariances are computed from a weighted dataset in EM. In this
            case we can compute the number of effective samples by
            `np.sum(weights, axis=0) ** 2 / np.sum(weights ** 2, axis=0)`
            from an array weights of shape (n_samples, n_components).
        &#34;&#34;&#34;
        self._check_initialized()
        n_samples_eff = np.ones(self.n_components) * np.asarray(n_samples_eff)

        n_features = self.means.shape[1]
        for k in range(self.n_components):
            emp_cov = self.covariances[k]
            mu = np.trace(emp_cov) / n_features
            alpha = np.mean(emp_cov ** 2)
            num = alpha + mu ** 2
            den = (n_samples_eff[k] + 1.) * (alpha - (mu ** 2) / n_features)

            shrinkage = 1. if den == 0 else min(num / den, 1.)
            shrunk_cov = (1. - shrinkage) * emp_cov
            shrunk_cov.flat[::n_features + 1] += shrinkage * mu

            self.covariances[k] = shrunk_cov

    def sample(self, n_samples):
        &#34;&#34;&#34;Sample from Gaussian mixture distribution.

        Parameters
        ----------
        n_samples : int
            Number of samples.

        Returns
        -------
        X : array, shape (n_samples, n_features)
            Samples from the GMM.
        &#34;&#34;&#34;
        self._check_initialized()

        mvn_indices = self.random_state.choice(
            self.n_components, size=(n_samples,), p=self.priors)
        mvn_indices.sort()
        split_indices = np.hstack(
            ((0,), np.nonzero(np.diff(mvn_indices))[0] + 1, (n_samples,)))
        clusters = np.unique(mvn_indices)
        lens = np.diff(split_indices)
        samples = np.empty((n_samples, self.means.shape[1]))
        for i, (k, n_samples) in enumerate(zip(clusters, lens)):
            samples[split_indices[i]:split_indices[i + 1]] = MVN(
                mean=self.means[k], covariance=self.covariances[k],
                random_state=self.random_state).sample(n_samples=n_samples)
        return samples

    def sample_confidence_region(self, n_samples, alpha):
        &#34;&#34;&#34;Sample from alpha confidence region.

        Each MVN is selected with its prior probability and then we
        sample from the confidence region of the selected MVN.

        Parameters
        ----------
        n_samples : int
            Number of samples.

        alpha : float
            Value between 0 and 1 that defines the probability of the
            confidence region, e.g., 0.6827 for the 1-sigma confidence
            region or 0.9545 for the 2-sigma confidence region.

        Returns
        -------
        X : array, shape (n_samples, n_features)
            Samples from the confidence region.
        &#34;&#34;&#34;
        self._check_initialized()

        mvn_indices = self.random_state.choice(
            self.n_components, size=(n_samples,), p=self.priors)
        mvn_indices.sort()
        split_indices = np.hstack(
            ((0,), np.nonzero(np.diff(mvn_indices))[0] + 1, (n_samples,)))
        clusters = np.unique(mvn_indices)
        lens = np.diff(split_indices)
        samples = np.empty((n_samples, self.means.shape[1]))
        for i, (k, n_samples) in enumerate(zip(clusters, lens)):
            samples[split_indices[i]:split_indices[i + 1]] = MVN(
                mean=self.means[k], covariance=self.covariances[k],
                random_state=self.random_state).sample_confidence_region(
                n_samples=n_samples, alpha=alpha)
        return samples

    def is_in_confidence_region(self, x, alpha):
        &#34;&#34;&#34;Check if sample is in alpha confidence region.

        Check whether the sample lies in the confidence region of the closest
        MVN according to the Mahalanobis distance.

        Parameters
        ----------
        x : array, shape (n_features,)
            Sample

        alpha : float
            Value between 0 and 1 that defines the probability of the
            confidence region, e.g., 0.6827 for the 1-sigma confidence
            region or 0.9545 for the 2-sigma confidence region.

        Returns
        -------
        is_in_confidence_region : bool
            Is the sample in the alpha confidence region?
        &#34;&#34;&#34;
        self._check_initialized()
        dists = [MVN(mean=self.means[k], covariance=self.covariances[k]
                     ).squared_mahalanobis_distance(x)
                 for k in range(self.n_components)]
        # we have one degree of freedom less than number of dimensions
        n_dof = len(x) - 1
        if n_dof &gt;= 1:
            return min(dists) &lt;= chi2(n_dof).ppf(alpha)
        else:  # 1D
            idx = np.argmin(dists)
            lo, hi = norm.interval(
                alpha, loc=self.means[idx, 0],
                scale=self.covariances[idx, 0, 0])
            return lo &lt;= x[0] &lt;= hi

    def to_responsibilities(self, X):
        &#34;&#34;&#34;Compute responsibilities of each MVN for each sample.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Data.

        Returns
        -------
        R : array, shape (n_samples, n_components)
        &#34;&#34;&#34;
        self._check_initialized()

        n_samples = X.shape[0]

        norm_factors = np.empty(self.n_components)
        exponents = np.empty((n_samples, self.n_components))

        for k in range(self.n_components):
            norm_factors[k], exponents[:, k] = MVN(
                mean=self.means[k], covariance=self.covariances[k],
                random_state=self.random_state).to_norm_factor_and_exponents(X)

        return _safe_probability_density(self.priors * norm_factors, exponents)

    def to_probability_density(self, X):
        &#34;&#34;&#34;Compute probability density.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Data.

        Returns
        -------
        p : array, shape (n_samples,)
            Probability densities of data.
        &#34;&#34;&#34;
        self._check_initialized()

        p = [MVN(mean=self.means[k], covariance=self.covariances[k],
                 random_state=self.random_state).to_probability_density(X)
             for k in range(self.n_components)]
        return np.dot(self.priors, p)

    def condition(self, indices, x):
        &#34;&#34;&#34;Conditional distribution over given indices.

        Parameters
        ----------
        indices : array-like, shape (n_new_features,)
            Indices of dimensions that we want to condition.

        x : array-like, shape (n_new_features,)
            Values of the features that we know.

        Returns
        -------
        conditional : GMM
            Conditional GMM distribution p(Y | X=x).
        &#34;&#34;&#34;
        self._check_initialized()

        indices = np.asarray(indices, dtype=int)
        x = np.asarray(x)

        n_features = self.means.shape[1] - len(indices)
        means = np.empty((self.n_components, n_features))
        covariances = np.empty((self.n_components, n_features, n_features))

        marginal_norm_factors = np.empty(self.n_components)
        marginal_prior_exponents = np.empty(self.n_components)

        for k in range(self.n_components):
            mvn = MVN(mean=self.means[k], covariance=self.covariances[k],
                      random_state=self.random_state)
            conditioned = mvn.condition(indices, x)
            means[k] = conditioned.mean
            covariances[k] = conditioned.covariance

            marginal_norm_factors[k], marginal_prior_exponents[k] = \
                mvn.marginalize(indices).to_norm_factor_and_exponents(x)

        priors = _safe_probability_density(
            self.priors * marginal_norm_factors,
            marginal_prior_exponents[np.newaxis])[0]

        return GMM(n_components=self.n_components, priors=priors, means=means,
                   covariances=covariances, random_state=self.random_state)

    def predict(self, indices, X):
        &#34;&#34;&#34;Predict means of posteriors.

        Same as condition() but for multiple samples.

        Parameters
        ----------
        indices : array-like, shape (n_features_in,)
            Indices of dimensions that we want to condition.

        X : array-like, shape (n_samples, n_features_in)
            Values of the features that we know.

        Returns
        -------
        Y : array, shape (n_samples, n_features_out)
            Predicted means of missing values.
        &#34;&#34;&#34;
        self._check_initialized()

        indices = np.asarray(indices, dtype=int)
        X = np.asarray(X)

        n_samples = len(X)
        output_indices = invert_indices(self.means.shape[1], indices)
        regression_coeffs = np.empty((
            self.n_components, len(output_indices), len(indices)))

        marginal_norm_factors = np.empty(self.n_components)
        marginal_exponents = np.empty((n_samples, self.n_components))

        for k in range(self.n_components):
            regression_coeffs[k] = regression_coefficients(
                self.covariances[k], output_indices, indices)
            mvn = MVN(mean=self.means[k], covariance=self.covariances[k],
                      random_state=self.random_state)
            marginal_norm_factors[k], marginal_exponents[:, k] = \
                mvn.marginalize(indices).to_norm_factor_and_exponents(X)

        # posterior_means = mean_y + cov_xx^-1 * cov_xy * (x - mean_x)
        posterior_means = (
                self.means[:, output_indices][:, :, np.newaxis].T +
                np.einsum(
                    &#34;ijk,lik-&gt;lji&#34;,
                    regression_coeffs,
                    X[:, np.newaxis] - self.means[:, indices]))

        priors = _safe_probability_density(
            self.priors * marginal_norm_factors, marginal_exponents)
        priors = priors.reshape(n_samples, 1, self.n_components)
        return np.sum(priors * posterior_means, axis=-1)

    def to_ellipses(self, factor=1.0):
        &#34;&#34;&#34;Compute error ellipses.

        An error ellipse shows equiprobable points.

        Parameters
        ----------
        factor : float
            One means standard deviation.

        Returns
        -------
        ellipses : list
            Parameters that describe the error ellipses of all components:
            mean and a tuple of angles, widths and heights. Note that widths
            and heights are semi axes, not diameters.
        &#34;&#34;&#34;
        self._check_initialized()

        res = []
        for k in range(self.n_components):
            mvn = MVN(mean=self.means[k], covariance=self.covariances[k],
                      random_state=self.random_state)
            res.append((self.means[k], mvn.to_ellipse(factor)))
        return res

    def to_mvn(self):
        &#34;&#34;&#34;Collapse to a single Gaussian.

        Returns
        -------
        mvn : MVN
            Multivariate normal distribution.
        &#34;&#34;&#34;
        self._check_initialized()

        mean = np.sum(self.priors[:, np.newaxis] * self.means, 0)
        assert len(self.covariances)
        covariance = np.zeros_like(self.covariances[0])
        covariance += np.sum(self.priors[:, np.newaxis, np.newaxis] * self.covariances, axis=0)
        covariance += self.means.T.dot(np.diag(self.priors)).dot(self.means)
        covariance -= np.outer(mean, mean)
        return MVN(mean=mean, covariance=covariance,
                   verbose=self.verbose, random_state=self.random_state)

    def extract_mvn(self, component_idx):
        &#34;&#34;&#34;Extract one of the Gaussians from the mixture.

        Parameters
        ----------
        component_idx : int
            Index of the component that should be extracted.

        Returns
        -------
        mvn : MVN
            The component_idx-th multivariate normal distribution of this GMM.
        &#34;&#34;&#34;
        self._check_initialized()
        if component_idx &lt; 0 or component_idx &gt;= self.n_components:
            raise ValueError(&#34;Index of Gaussian must be in [%d, %d)&#34;
                             % (0, self.n_components))
        return MVN(
            mean=self.means[component_idx],
            covariance=self.covariances[component_idx], verbose=self.verbose,
            random_state=self.random_state)</code></pre>
</details>
<div class="desc"><p>Gaussian Mixture Model.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_components</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of MVNs that compose the GMM.</dd>
<dt><strong><code>priors</code></strong> :&ensp;<code>array-like, shape (n_components,)</code>, optional</dt>
<dd>Weights of the components.</dd>
<dt><strong><code>means</code></strong> :&ensp;<code>array-like, shape (n_components, n_features)</code>, optional</dt>
<dd>Means of the components.</dd>
<dt><strong><code>covariances</code></strong> :&ensp;<code>array-like, shape (n_components, n_features, n_features)</code>, optional</dt>
<dd>Covariances of the components.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int</code>, optional <code>(default: 0)</code></dt>
<dd>Verbosity level.</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code> or <code>RandomState</code>, optional <code>(default: global random state)</code></dt>
<dd>If an integer is given, it fixes the seed. Defaults to the global numpy
random number generator.</dd>
</dl></div>
<h3>Methods</h3>
<dl>
<dt id="gmr.GMM.apply_oracle_approximating_shrinkage"><code class="name flex">
<span>def <span class="ident">apply_oracle_approximating_shrinkage</span></span>(<span>self, n_samples_eff)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_oracle_approximating_shrinkage(self, n_samples_eff):
    &#34;&#34;&#34;Apply Oracle Approximating Shrinkage to covariances.

    Empirical covariances might have negative eigenvalues caused by
    numerical issues so that they are not positive semi-definite and
    are not invertible. In these cases it is better to apply this
    function after training. You can also apply it after each
    EM step with a flag of `GMM.from_samples`.

    This is an implementation of

    Chen et al., “Shrinkage Algorithms for MMSE Covariance Estimation”,
    IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.

    based on the implementation of scikit-learn:

    https://scikit-learn.org/stable/modules/generated/oas-function.html#sklearn.covariance.oas

    Parameters
    ----------
    n_samples_eff : float or array-like, shape (n_components,)
        Number of effective samples from which the covariances have been
        estimated. A rough estimate would be the size of the dataset.
        Covariances are computed from a weighted dataset in EM. In this
        case we can compute the number of effective samples by
        `np.sum(weights, axis=0) ** 2 / np.sum(weights ** 2, axis=0)`
        from an array weights of shape (n_samples, n_components).
    &#34;&#34;&#34;
    self._check_initialized()
    n_samples_eff = np.ones(self.n_components) * np.asarray(n_samples_eff)

    n_features = self.means.shape[1]
    for k in range(self.n_components):
        emp_cov = self.covariances[k]
        mu = np.trace(emp_cov) / n_features
        alpha = np.mean(emp_cov ** 2)
        num = alpha + mu ** 2
        den = (n_samples_eff[k] + 1.) * (alpha - (mu ** 2) / n_features)

        shrinkage = 1. if den == 0 else min(num / den, 1.)
        shrunk_cov = (1. - shrinkage) * emp_cov
        shrunk_cov.flat[::n_features + 1] += shrinkage * mu

        self.covariances[k] = shrunk_cov</code></pre>
</details>
<div class="desc"><p>Apply Oracle Approximating Shrinkage to covariances.</p>
<p>Empirical covariances might have negative eigenvalues caused by
numerical issues so that they are not positive semi-definite and
are not invertible. In these cases it is better to apply this
function after training. You can also apply it after each
EM step with a flag of <code><a title="gmr.GMM.from_samples" href="#gmr.GMM.from_samples">GMM.from_samples()</a></code>.</p>
<p>This is an implementation of</p>
<p>Chen et al., “Shrinkage Algorithms for MMSE Covariance Estimation”,
IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.</p>
<p>based on the implementation of scikit-learn:</p>
<p><a href="https://scikit-learn.org/stable/modules/generated/oas-function.html#sklearn.covariance.oas">https://scikit-learn.org/stable/modules/generated/oas-function.html#sklearn.covariance.oas</a></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_samples_eff</code></strong> :&ensp;<code>float</code> or <code>array-like, shape (n_components,)</code></dt>
<dd>Number of effective samples from which the covariances have been
estimated. A rough estimate would be the size of the dataset.
Covariances are computed from a weighted dataset in EM. In this
case we can compute the number of effective samples by
<code>np.sum(weights, axis=0) ** 2 / np.sum(weights ** 2, axis=0)</code>
from an array weights of shape (n_samples, n_components).</dd>
</dl></div>
</dd>
<dt id="gmr.GMM.condition"><code class="name flex">
<span>def <span class="ident">condition</span></span>(<span>self, indices, x)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def condition(self, indices, x):
    &#34;&#34;&#34;Conditional distribution over given indices.

    Parameters
    ----------
    indices : array-like, shape (n_new_features,)
        Indices of dimensions that we want to condition.

    x : array-like, shape (n_new_features,)
        Values of the features that we know.

    Returns
    -------
    conditional : GMM
        Conditional GMM distribution p(Y | X=x).
    &#34;&#34;&#34;
    self._check_initialized()

    indices = np.asarray(indices, dtype=int)
    x = np.asarray(x)

    n_features = self.means.shape[1] - len(indices)
    means = np.empty((self.n_components, n_features))
    covariances = np.empty((self.n_components, n_features, n_features))

    marginal_norm_factors = np.empty(self.n_components)
    marginal_prior_exponents = np.empty(self.n_components)

    for k in range(self.n_components):
        mvn = MVN(mean=self.means[k], covariance=self.covariances[k],
                  random_state=self.random_state)
        conditioned = mvn.condition(indices, x)
        means[k] = conditioned.mean
        covariances[k] = conditioned.covariance

        marginal_norm_factors[k], marginal_prior_exponents[k] = \
            mvn.marginalize(indices).to_norm_factor_and_exponents(x)

    priors = _safe_probability_density(
        self.priors * marginal_norm_factors,
        marginal_prior_exponents[np.newaxis])[0]

    return GMM(n_components=self.n_components, priors=priors, means=means,
               covariances=covariances, random_state=self.random_state)</code></pre>
</details>
<div class="desc"><p>Conditional distribution over given indices.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>indices</code></strong> :&ensp;<code>array-like, shape (n_new_features,)</code></dt>
<dd>Indices of dimensions that we want to condition.</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>array-like, shape (n_new_features,)</code></dt>
<dd>Values of the features that we know.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>conditional</code></strong> :&ensp;<code><a title="gmr.GMM" href="#gmr.GMM">GMM</a></code></dt>
<dd>Conditional GMM distribution p(Y | X=x).</dd>
</dl></div>
</dd>
<dt id="gmr.GMM.extract_mvn"><code class="name flex">
<span>def <span class="ident">extract_mvn</span></span>(<span>self, component_idx)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_mvn(self, component_idx):
    &#34;&#34;&#34;Extract one of the Gaussians from the mixture.

    Parameters
    ----------
    component_idx : int
        Index of the component that should be extracted.

    Returns
    -------
    mvn : MVN
        The component_idx-th multivariate normal distribution of this GMM.
    &#34;&#34;&#34;
    self._check_initialized()
    if component_idx &lt; 0 or component_idx &gt;= self.n_components:
        raise ValueError(&#34;Index of Gaussian must be in [%d, %d)&#34;
                         % (0, self.n_components))
    return MVN(
        mean=self.means[component_idx],
        covariance=self.covariances[component_idx], verbose=self.verbose,
        random_state=self.random_state)</code></pre>
</details>
<div class="desc"><p>Extract one of the Gaussians from the mixture.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>component_idx</code></strong> :&ensp;<code>int</code></dt>
<dd>Index of the component that should be extracted.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>mvn</code></strong> :&ensp;<code><a title="gmr.MVN" href="#gmr.MVN">MVN</a></code></dt>
<dd>The component_idx-th multivariate normal distribution of this GMM.</dd>
</dl></div>
</dd>
<dt id="gmr.GMM.from_samples"><code class="name flex">
<span>def <span class="ident">from_samples</span></span>(<span>self,<br>X,<br>R_diff=0.0001,<br>n_iter=100,<br>init_params='random',<br>oracle_approximating_shrinkage=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def from_samples(self, X, R_diff=1e-4, n_iter=100, init_params=&#34;random&#34;,
                 oracle_approximating_shrinkage=False):
    &#34;&#34;&#34;MLE of the mean and covariance.

    Expectation-maximization is used to infer the model parameters. The
    objective function is non-convex. Hence, multiple runs can have
    different results.

    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        Samples from the true distribution.

    R_diff : float
        Minimum allowed difference of responsibilities between successive
        EM iterations.

    n_iter : int
        Maximum number of iterations.

    init_params : str, optional (default: &#39;random&#39;)
        Parameter initialization strategy. If means and covariances are
        given in the constructor, this parameter will have no effect.
        &#39;random&#39; will sample initial means randomly from the dataset
        and set covariances to identity matrices. This is the
        computationally cheap solution.
        &#39;kmeans++&#39; will use k-means++ initialization for means and
        initialize covariances to diagonal matrices with variances
        set based on the average distances of samples in each dimensions.
        This is computationally more expensive but often gives much
        better results.

    oracle_approximating_shrinkage : bool, optional (default: False)
        Use Oracle Approximating Shrinkage (OAS) estimator for covariances
        to ensure positive semi-definiteness.

    Returns
    -------
    self : GMM
        This object.
    &#34;&#34;&#34;
    n_samples, n_features = X.shape

    if self.priors is None:
        self.priors = np.ones(self.n_components,
                              dtype=float) / self.n_components

    if init_params not in [&#34;random&#34;, &#34;kmeans++&#34;]:
        raise ValueError(&#34;&#39;init_params&#39; must be &#39;random&#39; or &#39;kmeans++&#39; &#34;
                         &#34;but is &#39;%s&#39;&#34; % init_params)

    if self.means is None:
        if init_params == &#34;random&#34;:
            indices = self.random_state.choice(
                np.arange(n_samples), self.n_components)
            self.means = X[indices]
        else:
            self.means = kmeansplusplus_initialization(
                X, self.n_components, self.random_state)

    if self.covariances is None:
        if init_params == &#34;random&#34;:
            self.covariances = np.empty(
                (self.n_components, n_features, n_features))
            self.covariances[:] = np.eye(n_features)
        else:
            self.covariances = covariance_initialization(
                X, self.n_components)

    R = np.zeros((n_samples, self.n_components))
    for _ in range(n_iter):
        R_prev = R

        # Expectation
        R = self.to_responsibilities(X)

        if np.linalg.norm(R - R_prev) &lt; R_diff:
            if self.verbose:
                print(&#34;EM converged.&#34;)
            break

        # Maximization
        w = R.sum(axis=0) + 10.0 * np.finfo(R.dtype).eps
        R_n = R / w
        self.priors = w / w.sum()
        self.means = R_n.T.dot(X)
        for k in range(self.n_components):
            Xm = X - self.means[k]
            self.covariances[k] = (R_n[:, k, np.newaxis] * Xm).T.dot(Xm)

        if oracle_approximating_shrinkage:
            n_samples_eff = (np.sum(R_n, axis=0) ** 2 /
                             np.sum(R_n ** 2, axis=0))
            self.apply_oracle_approximating_shrinkage(n_samples_eff)

    return self</code></pre>
</details>
<div class="desc"><p>MLE of the mean and covariance.</p>
<p>Expectation-maximization is used to infer the model parameters. The
objective function is non-convex. Hence, multiple runs can have
different results.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like, shape (n_samples, n_features)</code></dt>
<dd>Samples from the true distribution.</dd>
<dt><strong><code>R_diff</code></strong> :&ensp;<code>float</code></dt>
<dd>Minimum allowed difference of responsibilities between successive
EM iterations.</dd>
<dt><strong><code>n_iter</code></strong> :&ensp;<code>int</code></dt>
<dd>Maximum number of iterations.</dd>
<dt><strong><code>init_params</code></strong> :&ensp;<code>str</code>, optional <code>(default: 'random')</code></dt>
<dd>Parameter initialization strategy. If means and covariances are
given in the constructor, this parameter will have no effect.
'random' will sample initial means randomly from the dataset
and set covariances to identity matrices. This is the
computationally cheap solution.
'kmeans++' will use k-means++ initialization for means and
initialize covariances to diagonal matrices with variances
set based on the average distances of samples in each dimensions.
This is computationally more expensive but often gives much
better results.</dd>
<dt><strong><code>oracle_approximating_shrinkage</code></strong> :&ensp;<code>bool</code>, optional <code>(default: False)</code></dt>
<dd>Use Oracle Approximating Shrinkage (OAS) estimator for covariances
to ensure positive semi-definiteness.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code><a title="gmr.GMM" href="#gmr.GMM">GMM</a></code></dt>
<dd>This object.</dd>
</dl></div>
</dd>
<dt id="gmr.GMM.is_in_confidence_region"><code class="name flex">
<span>def <span class="ident">is_in_confidence_region</span></span>(<span>self, x, alpha)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_in_confidence_region(self, x, alpha):
    &#34;&#34;&#34;Check if sample is in alpha confidence region.

    Check whether the sample lies in the confidence region of the closest
    MVN according to the Mahalanobis distance.

    Parameters
    ----------
    x : array, shape (n_features,)
        Sample

    alpha : float
        Value between 0 and 1 that defines the probability of the
        confidence region, e.g., 0.6827 for the 1-sigma confidence
        region or 0.9545 for the 2-sigma confidence region.

    Returns
    -------
    is_in_confidence_region : bool
        Is the sample in the alpha confidence region?
    &#34;&#34;&#34;
    self._check_initialized()
    dists = [MVN(mean=self.means[k], covariance=self.covariances[k]
                 ).squared_mahalanobis_distance(x)
             for k in range(self.n_components)]
    # we have one degree of freedom less than number of dimensions
    n_dof = len(x) - 1
    if n_dof &gt;= 1:
        return min(dists) &lt;= chi2(n_dof).ppf(alpha)
    else:  # 1D
        idx = np.argmin(dists)
        lo, hi = norm.interval(
            alpha, loc=self.means[idx, 0],
            scale=self.covariances[idx, 0, 0])
        return lo &lt;= x[0] &lt;= hi</code></pre>
</details>
<div class="desc"><p>Check if sample is in alpha confidence region.</p>
<p>Check whether the sample lies in the confidence region of the closest
MVN according to the Mahalanobis distance.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>array, shape (n_features,)</code></dt>
<dd>Sample</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>Value between 0 and 1 that defines the probability of the
confidence region, e.g., 0.6827 for the 1-sigma confidence
region or 0.9545 for the 2-sigma confidence region.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>is_in_confidence_region</code></strong> :&ensp;<code>bool</code></dt>
<dd>Is the sample in the alpha confidence region?</dd>
</dl></div>
</dd>
<dt id="gmr.GMM.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, indices, X)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, indices, X):
    &#34;&#34;&#34;Predict means of posteriors.

    Same as condition() but for multiple samples.

    Parameters
    ----------
    indices : array-like, shape (n_features_in,)
        Indices of dimensions that we want to condition.

    X : array-like, shape (n_samples, n_features_in)
        Values of the features that we know.

    Returns
    -------
    Y : array, shape (n_samples, n_features_out)
        Predicted means of missing values.
    &#34;&#34;&#34;
    self._check_initialized()

    indices = np.asarray(indices, dtype=int)
    X = np.asarray(X)

    n_samples = len(X)
    output_indices = invert_indices(self.means.shape[1], indices)
    regression_coeffs = np.empty((
        self.n_components, len(output_indices), len(indices)))

    marginal_norm_factors = np.empty(self.n_components)
    marginal_exponents = np.empty((n_samples, self.n_components))

    for k in range(self.n_components):
        regression_coeffs[k] = regression_coefficients(
            self.covariances[k], output_indices, indices)
        mvn = MVN(mean=self.means[k], covariance=self.covariances[k],
                  random_state=self.random_state)
        marginal_norm_factors[k], marginal_exponents[:, k] = \
            mvn.marginalize(indices).to_norm_factor_and_exponents(X)

    # posterior_means = mean_y + cov_xx^-1 * cov_xy * (x - mean_x)
    posterior_means = (
            self.means[:, output_indices][:, :, np.newaxis].T +
            np.einsum(
                &#34;ijk,lik-&gt;lji&#34;,
                regression_coeffs,
                X[:, np.newaxis] - self.means[:, indices]))

    priors = _safe_probability_density(
        self.priors * marginal_norm_factors, marginal_exponents)
    priors = priors.reshape(n_samples, 1, self.n_components)
    return np.sum(priors * posterior_means, axis=-1)</code></pre>
</details>
<div class="desc"><p>Predict means of posteriors.</p>
<p>Same as condition() but for multiple samples.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>indices</code></strong> :&ensp;<code>array-like, shape (n_features_in,)</code></dt>
<dd>Indices of dimensions that we want to condition.</dd>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like, shape (n_samples, n_features_in)</code></dt>
<dd>Values of the features that we know.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>Y</code></strong> :&ensp;<code>array, shape (n_samples, n_features_out)</code></dt>
<dd>Predicted means of missing values.</dd>
</dl></div>
</dd>
<dt id="gmr.GMM.sample"><code class="name flex">
<span>def <span class="ident">sample</span></span>(<span>self, n_samples)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample(self, n_samples):
    &#34;&#34;&#34;Sample from Gaussian mixture distribution.

    Parameters
    ----------
    n_samples : int
        Number of samples.

    Returns
    -------
    X : array, shape (n_samples, n_features)
        Samples from the GMM.
    &#34;&#34;&#34;
    self._check_initialized()

    mvn_indices = self.random_state.choice(
        self.n_components, size=(n_samples,), p=self.priors)
    mvn_indices.sort()
    split_indices = np.hstack(
        ((0,), np.nonzero(np.diff(mvn_indices))[0] + 1, (n_samples,)))
    clusters = np.unique(mvn_indices)
    lens = np.diff(split_indices)
    samples = np.empty((n_samples, self.means.shape[1]))
    for i, (k, n_samples) in enumerate(zip(clusters, lens)):
        samples[split_indices[i]:split_indices[i + 1]] = MVN(
            mean=self.means[k], covariance=self.covariances[k],
            random_state=self.random_state).sample(n_samples=n_samples)
    return samples</code></pre>
</details>
<div class="desc"><p>Sample from Gaussian mixture distribution.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of samples.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array, shape (n_samples, n_features)</code></dt>
<dd>Samples from the GMM.</dd>
</dl></div>
</dd>
<dt id="gmr.GMM.sample_confidence_region"><code class="name flex">
<span>def <span class="ident">sample_confidence_region</span></span>(<span>self, n_samples, alpha)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_confidence_region(self, n_samples, alpha):
    &#34;&#34;&#34;Sample from alpha confidence region.

    Each MVN is selected with its prior probability and then we
    sample from the confidence region of the selected MVN.

    Parameters
    ----------
    n_samples : int
        Number of samples.

    alpha : float
        Value between 0 and 1 that defines the probability of the
        confidence region, e.g., 0.6827 for the 1-sigma confidence
        region or 0.9545 for the 2-sigma confidence region.

    Returns
    -------
    X : array, shape (n_samples, n_features)
        Samples from the confidence region.
    &#34;&#34;&#34;
    self._check_initialized()

    mvn_indices = self.random_state.choice(
        self.n_components, size=(n_samples,), p=self.priors)
    mvn_indices.sort()
    split_indices = np.hstack(
        ((0,), np.nonzero(np.diff(mvn_indices))[0] + 1, (n_samples,)))
    clusters = np.unique(mvn_indices)
    lens = np.diff(split_indices)
    samples = np.empty((n_samples, self.means.shape[1]))
    for i, (k, n_samples) in enumerate(zip(clusters, lens)):
        samples[split_indices[i]:split_indices[i + 1]] = MVN(
            mean=self.means[k], covariance=self.covariances[k],
            random_state=self.random_state).sample_confidence_region(
            n_samples=n_samples, alpha=alpha)
    return samples</code></pre>
</details>
<div class="desc"><p>Sample from alpha confidence region.</p>
<p>Each MVN is selected with its prior probability and then we
sample from the confidence region of the selected MVN.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of samples.</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>Value between 0 and 1 that defines the probability of the
confidence region, e.g., 0.6827 for the 1-sigma confidence
region or 0.9545 for the 2-sigma confidence region.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array, shape (n_samples, n_features)</code></dt>
<dd>Samples from the confidence region.</dd>
</dl></div>
</dd>
<dt id="gmr.GMM.to_ellipses"><code class="name flex">
<span>def <span class="ident">to_ellipses</span></span>(<span>self, factor=1.0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_ellipses(self, factor=1.0):
    &#34;&#34;&#34;Compute error ellipses.

    An error ellipse shows equiprobable points.

    Parameters
    ----------
    factor : float
        One means standard deviation.

    Returns
    -------
    ellipses : list
        Parameters that describe the error ellipses of all components:
        mean and a tuple of angles, widths and heights. Note that widths
        and heights are semi axes, not diameters.
    &#34;&#34;&#34;
    self._check_initialized()

    res = []
    for k in range(self.n_components):
        mvn = MVN(mean=self.means[k], covariance=self.covariances[k],
                  random_state=self.random_state)
        res.append((self.means[k], mvn.to_ellipse(factor)))
    return res</code></pre>
</details>
<div class="desc"><p>Compute error ellipses.</p>
<p>An error ellipse shows equiprobable points.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>factor</code></strong> :&ensp;<code>float</code></dt>
<dd>One means standard deviation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>ellipses</code></strong> :&ensp;<code>list</code></dt>
<dd>Parameters that describe the error ellipses of all components:
mean and a tuple of angles, widths and heights. Note that widths
and heights are semi axes, not diameters.</dd>
</dl></div>
</dd>
<dt id="gmr.GMM.to_mvn"><code class="name flex">
<span>def <span class="ident">to_mvn</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_mvn(self):
    &#34;&#34;&#34;Collapse to a single Gaussian.

    Returns
    -------
    mvn : MVN
        Multivariate normal distribution.
    &#34;&#34;&#34;
    self._check_initialized()

    mean = np.sum(self.priors[:, np.newaxis] * self.means, 0)
    assert len(self.covariances)
    covariance = np.zeros_like(self.covariances[0])
    covariance += np.sum(self.priors[:, np.newaxis, np.newaxis] * self.covariances, axis=0)
    covariance += self.means.T.dot(np.diag(self.priors)).dot(self.means)
    covariance -= np.outer(mean, mean)
    return MVN(mean=mean, covariance=covariance,
               verbose=self.verbose, random_state=self.random_state)</code></pre>
</details>
<div class="desc"><p>Collapse to a single Gaussian.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>mvn</code></strong> :&ensp;<code><a title="gmr.MVN" href="#gmr.MVN">MVN</a></code></dt>
<dd>Multivariate normal distribution.</dd>
</dl></div>
</dd>
<dt id="gmr.GMM.to_probability_density"><code class="name flex">
<span>def <span class="ident">to_probability_density</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_probability_density(self, X):
    &#34;&#34;&#34;Compute probability density.

    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        Data.

    Returns
    -------
    p : array, shape (n_samples,)
        Probability densities of data.
    &#34;&#34;&#34;
    self._check_initialized()

    p = [MVN(mean=self.means[k], covariance=self.covariances[k],
             random_state=self.random_state).to_probability_density(X)
         for k in range(self.n_components)]
    return np.dot(self.priors, p)</code></pre>
</details>
<div class="desc"><p>Compute probability density.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like, shape (n_samples, n_features)</code></dt>
<dd>Data.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>p</code></strong> :&ensp;<code>array, shape (n_samples,)</code></dt>
<dd>Probability densities of data.</dd>
</dl></div>
</dd>
<dt id="gmr.GMM.to_responsibilities"><code class="name flex">
<span>def <span class="ident">to_responsibilities</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_responsibilities(self, X):
    &#34;&#34;&#34;Compute responsibilities of each MVN for each sample.

    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        Data.

    Returns
    -------
    R : array, shape (n_samples, n_components)
    &#34;&#34;&#34;
    self._check_initialized()

    n_samples = X.shape[0]

    norm_factors = np.empty(self.n_components)
    exponents = np.empty((n_samples, self.n_components))

    for k in range(self.n_components):
        norm_factors[k], exponents[:, k] = MVN(
            mean=self.means[k], covariance=self.covariances[k],
            random_state=self.random_state).to_norm_factor_and_exponents(X)

    return _safe_probability_density(self.priors * norm_factors, exponents)</code></pre>
</details>
<div class="desc"><p>Compute responsibilities of each MVN for each sample.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like, shape (n_samples, n_features)</code></dt>
<dd>Data.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>R</code></strong> :&ensp;<code>array, shape (n_samples, n_components)</code></dt>
<dd>&nbsp;</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="gmr.MVN"><code class="flex name class">
<span>class <span class="ident">MVN</span></span>
<span>(</span><span>mean=None, covariance=None, verbose=0, random_state=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MVN(object):
    &#34;&#34;&#34;Multivariate normal distribution.

    Some utility functions for MVNs. See
    http://en.wikipedia.org/wiki/Multivariate_normal_distribution
    for more details.

    Parameters
    ----------
    mean : array-like, shape (n_features), optional
        Mean of the MVN.

    covariance : array-like, shape (n_features, n_features), optional
        Covariance of the MVN.

    verbose : int, optional (default: 0)
        Verbosity level.

    random_state : int or RandomState, optional (default: global random state)
        If an integer is given, it fixes the seed. Defaults to the global numpy
        random number generator.
    &#34;&#34;&#34;
    def __init__(self, mean=None, covariance=None, verbose=0,
                 random_state=None):
        self.mean = mean
        self.covariance = covariance
        self.verbose = verbose
        self.random_state = check_random_state(random_state)
        self.norm = None

        if self.mean is not None:
            self.mean = np.asarray(self.mean)
        if self.covariance is not None:
            self.covariance = np.asarray(self.covariance)

    def _check_initialized(self):
        if self.mean is None:
            raise ValueError(&#34;Mean has not been initialized&#34;)
        if self.covariance is None:
            raise ValueError(&#34;Covariance has not been initialized&#34;)

    def from_samples(self, X, bessels_correction=True):
        &#34;&#34;&#34;MLE of the mean and covariance.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Samples from the true function.

        bessels_correction : bool
            Apply Bessel&#39;s correction to the covariance estimate.

        Returns
        -------
        self : MVN
            This object.
        &#34;&#34;&#34;
        self.mean = np.mean(X, axis=0)
        bias = 0 if bessels_correction else 1
        self.covariance = np.cov(X, rowvar=0, bias=bias)
        self.norm = None
        return self

    def sample(self, n_samples):
        &#34;&#34;&#34;Sample from multivariate normal distribution.

        Parameters
        ----------
        n_samples : int
            Number of samples.

        Returns
        -------
        X : array, shape (n_samples, n_features)
            Samples from the MVN.
        &#34;&#34;&#34;
        self._check_initialized()
        return self.random_state.multivariate_normal(
            self.mean, self.covariance, size=(n_samples,))

    def sample_confidence_region(self, n_samples, alpha):
        &#34;&#34;&#34;Sample from alpha confidence region.

        Parameters
        ----------
        n_samples : int
            Number of samples.

        alpha : float
            Value between 0 and 1 that defines the probability of the
            confidence region, e.g., 0.6827 for the 1-sigma confidence
            region or 0.9545 for the 2-sigma confidence region.

        Returns
        -------
        X : array, shape (n_samples, n_features)
            Samples from the confidence region.
        &#34;&#34;&#34;
        return np.array([self._one_sample_confidence_region(alpha)
                         for _ in range(n_samples)])

    def _one_sample_confidence_region(self, alpha):
        x = self.sample(1)[0]
        while not self.is_in_confidence_region(x, alpha):
            x = self.sample(1)[0]
        return x

    def is_in_confidence_region(self, x, alpha):
        &#34;&#34;&#34;Check if sample is in alpha confidence region.

        Parameters
        ----------
        x : array, shape (n_features,)
            Sample

        alpha : float
            Value between 0 and 1 that defines the probability of the
            confidence region, e.g., 0.6827 for the 1-sigma confidence
            region or 0.9545 for the 2-sigma confidence region.

        Returns
        -------
        is_in_confidence_region : bool
            Is the sample in the alpha confidence region?
        &#34;&#34;&#34;
        self._check_initialized()
        # we have one degree of freedom less than number of dimensions
        n_dof = len(x) - 1
        if n_dof &gt;= 1:
            return self.squared_mahalanobis_distance(x) &lt;= chi2(n_dof).ppf(alpha)
        else:  # 1D
            lo, hi = norm.interval(
                alpha, loc=self.mean[0], scale=self.covariance[0, 0])
            return lo &lt;= x[0] &lt;= hi

    def to_norm_factor_and_exponents(self, X):
        &#34;&#34;&#34;Compute normalization factor and exponents of Gaussian.

        These values can be used to compute the probability density function
        of this Gaussian: p(x) = norm_factor * np.exp(exponents).

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Data.

        Returns
        -------
        norm_factor : float
            Normalization factor: constant term outside of exponential
            function in probability density function of this Gaussian.

        exponents : array, shape (n_samples,)
            Exponents to compute probability density function.
        &#34;&#34;&#34;
        self._check_initialized()

        X = np.atleast_2d(X)
        n_features = X.shape[1]

        try:
            L = sp.linalg.cholesky(self.covariance, lower=True)
        except np.linalg.LinAlgError:
            # Degenerated covariance, try to add regularization
            L = sp.linalg.cholesky(
                self.covariance + 1e-3 * np.eye(n_features), lower=True)

        X_minus_mean = X - self.mean

        if self.norm is None:
            # Suppress a determinant of 0 to avoid numerical problems
            L_det = max(sp.linalg.det(L), np.finfo(L.dtype).eps)
            self.norm = (2.0 * np.pi) ** (-0.5 * n_features) / L_det

        # Solve L x = (X - mean)^T for x with triangular L
        # (LL^T = Sigma), that is, x = L^T^-1 (X - mean)^T.
        # We can avoid covariance inversion when computing
        # (X - mean) Sigma^-1 (X - mean)^T  with this trick,
        # since Sigma^-1 = L^T^-1 L^-1.
        X_normalized = sp.linalg.solve_triangular(
            L, X_minus_mean.T, lower=True).T

        exponent = -0.5 * np.sum(X_normalized ** 2, axis=1)

        return self.norm, exponent

    def to_probability_density(self, X):
        &#34;&#34;&#34;Compute probability density.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Data.

        Returns
        -------
        p : array, shape (n_samples,)
            Probability densities of data.
        &#34;&#34;&#34;
        norm_factor, exponents = self.to_norm_factor_and_exponents(X)
        return norm_factor * np.exp(exponents)

    def marginalize(self, indices):
        &#34;&#34;&#34;Marginalize over everything except the given indices.

        Parameters
        ----------
        indices : array, shape (n_new_features,)
            Indices of dimensions that we want to keep.

        Returns
        -------
        marginal : MVN
            Marginal MVN distribution.
        &#34;&#34;&#34;
        self._check_initialized()
        return MVN(mean=self.mean[indices],
                   covariance=self.covariance[np.ix_(indices, indices)])

    def condition(self, indices, x):
        &#34;&#34;&#34;Conditional distribution over given indices.

        Parameters
        ----------
        indices : array, shape (n_new_features,)
            Indices of dimensions that we want to condition.

        x : array, shape (n_new_features,)
            Values of the features that we know.

        Returns
        -------
        conditional : MVN
            Conditional MVN distribution p(Y | X=x).
        &#34;&#34;&#34;
        self._check_initialized()
        mean, covariance = condition(
            self.mean, self.covariance,
            invert_indices(self.mean.shape[0], indices), indices, x)
        return MVN(mean=mean, covariance=covariance,
                   random_state=self.random_state)

    def predict(self, indices, X):
        &#34;&#34;&#34;Predict means and covariance of posteriors.

        Same as condition() but for multiple samples.

        Parameters
        ----------
        indices : array-like, shape (n_features_in,)
            Indices of dimensions that we want to condition.

        X : array-like, shape (n_samples, n_features_in)
            Values of the features that we know.

        Returns
        -------
        Y : array, shape (n_samples, n_features_out)
            Predicted means of missing values.

        covariance : array, shape (n_features_out, n_features_out)
            Covariance of the predicted features.
        &#34;&#34;&#34;
        self._check_initialized()
        indices = np.asarray(indices, dtype=int)
        X = np.asarray(X)
        return condition(
            self.mean, self.covariance,
            invert_indices(self.mean.shape[0], indices), indices, X)

    def squared_mahalanobis_distance(self, x):
        &#34;&#34;&#34;Squared Mahalanobis distance between point and this MVN.

        Parameters
        ----------
        x : array, shape (n_features,)

        Returns
        -------
        d : float
            Squared Mahalanobis distance
        &#34;&#34;&#34;
        self._check_initialized()
        return mahalanobis(x, self.mean, np.linalg.inv(self.covariance)) ** 2

    def to_ellipse(self, factor=1.0):
        &#34;&#34;&#34;Compute error ellipse.

        An error ellipse shows equiprobable points.

        Parameters
        ----------
        factor : float
            One means standard deviation.

        Returns
        -------
        angle : float
            Rotation angle of the ellipse.

        width : float
            Width of the ellipse (semi axis, not diameter).

        height : float
            Height of the ellipse (semi axis, not diameter).
        &#34;&#34;&#34;
        self._check_initialized()
        vals, vecs = sp.linalg.eigh(self.covariance)
        order = vals.argsort()[::-1]
        vals, vecs = vals[order], vecs[:, order]
        angle = np.arctan2(*vecs[:, 0][::-1])
        width, height = factor * np.sqrt(vals)
        return angle, width, height

    def _sqrt_cov(self, C):
        &#34;&#34;&#34;Compute square root of a symmetric matrix.

        Parameters
        ----------
        C : array, shape (n_features, n_features)
            Symmetric matrix.

        Returns
        -------
        sqrt(C) : array, shape (n_features, n_features)
            Square root of covariance. The square root of a square
            matrix is defined as
            :math:`\Sigma^{\frac{1}{2}} = B \sqrt(D) B^T`, where
            :math:`\Sigma = B D B^T` is the Eigen decomposition of the
            covariance.
        &#34;&#34;&#34;
        D, B = np.linalg.eigh(C)
        # HACK: avoid numerical problems
        D = np.maximum(D, np.finfo(float).eps)
        return B.dot(np.diag(np.sqrt(D))).dot(B.T)

    def sigma_points(self, alpha=1e-3, kappa=0.0):
        &#34;&#34;&#34;Compute sigma points for unscented transform.

        The unscented transform allows us to estimate the resulting MVN from
        applying a nonlinear transformation :math:`f` to this MVN. In order to
        do this, you have to transform the sigma points obtained from this
        function with :math:`f` and then create the new MVN with
        :func:`MVN.estimate_from_sigma_points`. The unscented transform is most
        commonly used in the Unscented Kalman Filter (UKF).

        Parameters
        ----------
        alpha : float, optional (default: 1e-3)
            Determines the spread of the sigma points around the mean and is
            usually set to a small positive value.

        kappa : float, optional (default: 0)
            A secondary scaling parameter which is usually set to 0.

        Returns
        -------
        sigma_points : array, shape (2 * n_features + 1, n_features)
            Query points that have to be transformed to estimate the resulting
            MVN.
        &#34;&#34;&#34;
        self._check_initialized()

        n_features = len(self.mean)
        lmbda = alpha ** 2 * (n_features + kappa) - n_features
        offset = self._sqrt_cov((n_features + lmbda) * self.covariance)

        points = np.empty(((2 * n_features + 1), n_features))
        points[0, :] = self.mean
        for i in range(n_features):
            points[1 + i, :] = self.mean + offset[i]
            points[1 + n_features + i:, :] = self.mean - offset[i]
        return points

    def estimate_from_sigma_points(self, transformed_sigma_points, alpha=1e-3, beta=2.0, kappa=0.0, random_state=None):
        &#34;&#34;&#34;Estimate new MVN from sigma points through the unscented transform.

        See :func:`MVN.sigma_points` for more details.

        Parameters
        ----------
        transformed_sigma_points : array, shape (2 * n_features + 1, n_features)
            Query points that were transformed to estimate the resulting MVN.

        alpha : float, optional (default: 1e-3)
            Determines the spread of the sigma points around the mean and is
            usually set to a small positive value. Note that this value has
            to match the value that was used to create the sigma points.

        beta : float, optional (default: 2)
            Encodes information about the distribution. For Gaussian
            distributions, beta=2 is the optimal choice.

        kappa : float, optional (default: 0)
            A secondary scaling parameter which is usually set to 0. Note that
            this value has to match the value that was used to create the
            sigma points.

        random_state : int or RandomState, optional (default: random state of self)
            If an integer is given, it fixes the seed. Defaults to the global
            numpy random number generator.

        Returns
        -------
        mvn : MVN
            Transformed MVN: f(self).
        &#34;&#34;&#34;
        self._check_initialized()

        n_features = len(self.mean)
        lmbda = alpha ** 2 * (n_features + kappa) - n_features

        mean_weight_0 = lmbda / (n_features + lmbda)
        cov_weight_0 = lmbda / (n_features + lmbda) + (1 - alpha ** 2 + beta)
        weights_i = 1.0 / (2.0 * (n_features + lmbda))
        mean_weights = np.empty(len(transformed_sigma_points))
        mean_weights[0] = mean_weight_0
        mean_weights[1:] = weights_i
        cov_weights = np.empty(len(transformed_sigma_points))
        cov_weights[0] = cov_weight_0
        cov_weights[1:] = weights_i

        mean = np.sum(mean_weights[:, np.newaxis] * transformed_sigma_points,
                      axis=0)
        sigma_points_minus_mean = transformed_sigma_points - mean
        covariance = sigma_points_minus_mean.T.dot(
            np.diag(cov_weights)).dot(sigma_points_minus_mean)

        if random_state is None:
            random_state = self.random_state
        return MVN(mean=mean, covariance=covariance, random_state=random_state)</code></pre>
</details>
<div class="desc"><p>Multivariate normal distribution.</p>
<p>Some utility functions for MVNs. See
<a href="http://en.wikipedia.org/wiki/Multivariate_normal_distribution">http://en.wikipedia.org/wiki/Multivariate_normal_distribution</a>
for more details.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mean</code></strong> :&ensp;<code>array-like, shape (n_features)</code>, optional</dt>
<dd>Mean of the MVN.</dd>
<dt><strong><code>covariance</code></strong> :&ensp;<code>array-like, shape (n_features, n_features)</code>, optional</dt>
<dd>Covariance of the MVN.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int</code>, optional <code>(default: 0)</code></dt>
<dd>Verbosity level.</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code> or <code>RandomState</code>, optional <code>(default: global random state)</code></dt>
<dd>If an integer is given, it fixes the seed. Defaults to the global numpy
random number generator.</dd>
</dl></div>
<h3>Methods</h3>
<dl>
<dt id="gmr.MVN.condition"><code class="name flex">
<span>def <span class="ident">condition</span></span>(<span>self, indices, x)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def condition(self, indices, x):
    &#34;&#34;&#34;Conditional distribution over given indices.

    Parameters
    ----------
    indices : array, shape (n_new_features,)
        Indices of dimensions that we want to condition.

    x : array, shape (n_new_features,)
        Values of the features that we know.

    Returns
    -------
    conditional : MVN
        Conditional MVN distribution p(Y | X=x).
    &#34;&#34;&#34;
    self._check_initialized()
    mean, covariance = condition(
        self.mean, self.covariance,
        invert_indices(self.mean.shape[0], indices), indices, x)
    return MVN(mean=mean, covariance=covariance,
               random_state=self.random_state)</code></pre>
</details>
<div class="desc"><p>Conditional distribution over given indices.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>indices</code></strong> :&ensp;<code>array, shape (n_new_features,)</code></dt>
<dd>Indices of dimensions that we want to condition.</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>array, shape (n_new_features,)</code></dt>
<dd>Values of the features that we know.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>conditional</code></strong> :&ensp;<code><a title="gmr.MVN" href="#gmr.MVN">MVN</a></code></dt>
<dd>Conditional MVN distribution p(Y | X=x).</dd>
</dl></div>
</dd>
<dt id="gmr.MVN.estimate_from_sigma_points"><code class="name flex">
<span>def <span class="ident">estimate_from_sigma_points</span></span>(<span>self, transformed_sigma_points, alpha=0.001, beta=2.0, kappa=0.0, random_state=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def estimate_from_sigma_points(self, transformed_sigma_points, alpha=1e-3, beta=2.0, kappa=0.0, random_state=None):
    &#34;&#34;&#34;Estimate new MVN from sigma points through the unscented transform.

    See :func:`MVN.sigma_points` for more details.

    Parameters
    ----------
    transformed_sigma_points : array, shape (2 * n_features + 1, n_features)
        Query points that were transformed to estimate the resulting MVN.

    alpha : float, optional (default: 1e-3)
        Determines the spread of the sigma points around the mean and is
        usually set to a small positive value. Note that this value has
        to match the value that was used to create the sigma points.

    beta : float, optional (default: 2)
        Encodes information about the distribution. For Gaussian
        distributions, beta=2 is the optimal choice.

    kappa : float, optional (default: 0)
        A secondary scaling parameter which is usually set to 0. Note that
        this value has to match the value that was used to create the
        sigma points.

    random_state : int or RandomState, optional (default: random state of self)
        If an integer is given, it fixes the seed. Defaults to the global
        numpy random number generator.

    Returns
    -------
    mvn : MVN
        Transformed MVN: f(self).
    &#34;&#34;&#34;
    self._check_initialized()

    n_features = len(self.mean)
    lmbda = alpha ** 2 * (n_features + kappa) - n_features

    mean_weight_0 = lmbda / (n_features + lmbda)
    cov_weight_0 = lmbda / (n_features + lmbda) + (1 - alpha ** 2 + beta)
    weights_i = 1.0 / (2.0 * (n_features + lmbda))
    mean_weights = np.empty(len(transformed_sigma_points))
    mean_weights[0] = mean_weight_0
    mean_weights[1:] = weights_i
    cov_weights = np.empty(len(transformed_sigma_points))
    cov_weights[0] = cov_weight_0
    cov_weights[1:] = weights_i

    mean = np.sum(mean_weights[:, np.newaxis] * transformed_sigma_points,
                  axis=0)
    sigma_points_minus_mean = transformed_sigma_points - mean
    covariance = sigma_points_minus_mean.T.dot(
        np.diag(cov_weights)).dot(sigma_points_minus_mean)

    if random_state is None:
        random_state = self.random_state
    return MVN(mean=mean, covariance=covariance, random_state=random_state)</code></pre>
</details>
<div class="desc"><p>Estimate new MVN from sigma points through the unscented transform.</p>
<p>See :func:<code><a title="gmr.MVN.sigma_points" href="#gmr.MVN.sigma_points">MVN.sigma_points()</a></code> for more details.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>transformed_sigma_points</code></strong> :&ensp;<code>array, shape (2 * n_features + 1, n_features)</code></dt>
<dd>Query points that were transformed to estimate the resulting MVN.</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code>, optional <code>(default: 1e-3)</code></dt>
<dd>Determines the spread of the sigma points around the mean and is
usually set to a small positive value. Note that this value has
to match the value that was used to create the sigma points.</dd>
<dt><strong><code>beta</code></strong> :&ensp;<code>float</code>, optional <code>(default: 2)</code></dt>
<dd>Encodes information about the distribution. For Gaussian
distributions, beta=2 is the optimal choice.</dd>
<dt><strong><code>kappa</code></strong> :&ensp;<code>float</code>, optional <code>(default: 0)</code></dt>
<dd>A secondary scaling parameter which is usually set to 0. Note that
this value has to match the value that was used to create the
sigma points.</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code> or <code>RandomState</code>, optional <code>(default: random state</code> of <code>self)</code></dt>
<dd>If an integer is given, it fixes the seed. Defaults to the global
numpy random number generator.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>mvn</code></strong> :&ensp;<code><a title="gmr.MVN" href="#gmr.MVN">MVN</a></code></dt>
<dd>Transformed MVN: f(self).</dd>
</dl></div>
</dd>
<dt id="gmr.MVN.from_samples"><code class="name flex">
<span>def <span class="ident">from_samples</span></span>(<span>self, X, bessels_correction=True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def from_samples(self, X, bessels_correction=True):
    &#34;&#34;&#34;MLE of the mean and covariance.

    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        Samples from the true function.

    bessels_correction : bool
        Apply Bessel&#39;s correction to the covariance estimate.

    Returns
    -------
    self : MVN
        This object.
    &#34;&#34;&#34;
    self.mean = np.mean(X, axis=0)
    bias = 0 if bessels_correction else 1
    self.covariance = np.cov(X, rowvar=0, bias=bias)
    self.norm = None
    return self</code></pre>
</details>
<div class="desc"><p>MLE of the mean and covariance.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like, shape (n_samples, n_features)</code></dt>
<dd>Samples from the true function.</dd>
<dt><strong><code>bessels_correction</code></strong> :&ensp;<code>bool</code></dt>
<dd>Apply Bessel's correction to the covariance estimate.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code><a title="gmr.MVN" href="#gmr.MVN">MVN</a></code></dt>
<dd>This object.</dd>
</dl></div>
</dd>
<dt id="gmr.MVN.is_in_confidence_region"><code class="name flex">
<span>def <span class="ident">is_in_confidence_region</span></span>(<span>self, x, alpha)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_in_confidence_region(self, x, alpha):
    &#34;&#34;&#34;Check if sample is in alpha confidence region.

    Parameters
    ----------
    x : array, shape (n_features,)
        Sample

    alpha : float
        Value between 0 and 1 that defines the probability of the
        confidence region, e.g., 0.6827 for the 1-sigma confidence
        region or 0.9545 for the 2-sigma confidence region.

    Returns
    -------
    is_in_confidence_region : bool
        Is the sample in the alpha confidence region?
    &#34;&#34;&#34;
    self._check_initialized()
    # we have one degree of freedom less than number of dimensions
    n_dof = len(x) - 1
    if n_dof &gt;= 1:
        return self.squared_mahalanobis_distance(x) &lt;= chi2(n_dof).ppf(alpha)
    else:  # 1D
        lo, hi = norm.interval(
            alpha, loc=self.mean[0], scale=self.covariance[0, 0])
        return lo &lt;= x[0] &lt;= hi</code></pre>
</details>
<div class="desc"><p>Check if sample is in alpha confidence region.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>array, shape (n_features,)</code></dt>
<dd>Sample</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>Value between 0 and 1 that defines the probability of the
confidence region, e.g., 0.6827 for the 1-sigma confidence
region or 0.9545 for the 2-sigma confidence region.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>is_in_confidence_region</code></strong> :&ensp;<code>bool</code></dt>
<dd>Is the sample in the alpha confidence region?</dd>
</dl></div>
</dd>
<dt id="gmr.MVN.marginalize"><code class="name flex">
<span>def <span class="ident">marginalize</span></span>(<span>self, indices)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def marginalize(self, indices):
    &#34;&#34;&#34;Marginalize over everything except the given indices.

    Parameters
    ----------
    indices : array, shape (n_new_features,)
        Indices of dimensions that we want to keep.

    Returns
    -------
    marginal : MVN
        Marginal MVN distribution.
    &#34;&#34;&#34;
    self._check_initialized()
    return MVN(mean=self.mean[indices],
               covariance=self.covariance[np.ix_(indices, indices)])</code></pre>
</details>
<div class="desc"><p>Marginalize over everything except the given indices.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>indices</code></strong> :&ensp;<code>array, shape (n_new_features,)</code></dt>
<dd>Indices of dimensions that we want to keep.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>marginal</code></strong> :&ensp;<code><a title="gmr.MVN" href="#gmr.MVN">MVN</a></code></dt>
<dd>Marginal MVN distribution.</dd>
</dl></div>
</dd>
<dt id="gmr.MVN.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, indices, X)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, indices, X):
    &#34;&#34;&#34;Predict means and covariance of posteriors.

    Same as condition() but for multiple samples.

    Parameters
    ----------
    indices : array-like, shape (n_features_in,)
        Indices of dimensions that we want to condition.

    X : array-like, shape (n_samples, n_features_in)
        Values of the features that we know.

    Returns
    -------
    Y : array, shape (n_samples, n_features_out)
        Predicted means of missing values.

    covariance : array, shape (n_features_out, n_features_out)
        Covariance of the predicted features.
    &#34;&#34;&#34;
    self._check_initialized()
    indices = np.asarray(indices, dtype=int)
    X = np.asarray(X)
    return condition(
        self.mean, self.covariance,
        invert_indices(self.mean.shape[0], indices), indices, X)</code></pre>
</details>
<div class="desc"><p>Predict means and covariance of posteriors.</p>
<p>Same as condition() but for multiple samples.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>indices</code></strong> :&ensp;<code>array-like, shape (n_features_in,)</code></dt>
<dd>Indices of dimensions that we want to condition.</dd>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like, shape (n_samples, n_features_in)</code></dt>
<dd>Values of the features that we know.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>Y</code></strong> :&ensp;<code>array, shape (n_samples, n_features_out)</code></dt>
<dd>Predicted means of missing values.</dd>
<dt><strong><code>covariance</code></strong> :&ensp;<code>array, shape (n_features_out, n_features_out)</code></dt>
<dd>Covariance of the predicted features.</dd>
</dl></div>
</dd>
<dt id="gmr.MVN.sample"><code class="name flex">
<span>def <span class="ident">sample</span></span>(<span>self, n_samples)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample(self, n_samples):
    &#34;&#34;&#34;Sample from multivariate normal distribution.

    Parameters
    ----------
    n_samples : int
        Number of samples.

    Returns
    -------
    X : array, shape (n_samples, n_features)
        Samples from the MVN.
    &#34;&#34;&#34;
    self._check_initialized()
    return self.random_state.multivariate_normal(
        self.mean, self.covariance, size=(n_samples,))</code></pre>
</details>
<div class="desc"><p>Sample from multivariate normal distribution.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of samples.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array, shape (n_samples, n_features)</code></dt>
<dd>Samples from the MVN.</dd>
</dl></div>
</dd>
<dt id="gmr.MVN.sample_confidence_region"><code class="name flex">
<span>def <span class="ident">sample_confidence_region</span></span>(<span>self, n_samples, alpha)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_confidence_region(self, n_samples, alpha):
    &#34;&#34;&#34;Sample from alpha confidence region.

    Parameters
    ----------
    n_samples : int
        Number of samples.

    alpha : float
        Value between 0 and 1 that defines the probability of the
        confidence region, e.g., 0.6827 for the 1-sigma confidence
        region or 0.9545 for the 2-sigma confidence region.

    Returns
    -------
    X : array, shape (n_samples, n_features)
        Samples from the confidence region.
    &#34;&#34;&#34;
    return np.array([self._one_sample_confidence_region(alpha)
                     for _ in range(n_samples)])</code></pre>
</details>
<div class="desc"><p>Sample from alpha confidence region.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of samples.</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>Value between 0 and 1 that defines the probability of the
confidence region, e.g., 0.6827 for the 1-sigma confidence
region or 0.9545 for the 2-sigma confidence region.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array, shape (n_samples, n_features)</code></dt>
<dd>Samples from the confidence region.</dd>
</dl></div>
</dd>
<dt id="gmr.MVN.sigma_points"><code class="name flex">
<span>def <span class="ident">sigma_points</span></span>(<span>self, alpha=0.001, kappa=0.0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sigma_points(self, alpha=1e-3, kappa=0.0):
    &#34;&#34;&#34;Compute sigma points for unscented transform.

    The unscented transform allows us to estimate the resulting MVN from
    applying a nonlinear transformation :math:`f` to this MVN. In order to
    do this, you have to transform the sigma points obtained from this
    function with :math:`f` and then create the new MVN with
    :func:`MVN.estimate_from_sigma_points`. The unscented transform is most
    commonly used in the Unscented Kalman Filter (UKF).

    Parameters
    ----------
    alpha : float, optional (default: 1e-3)
        Determines the spread of the sigma points around the mean and is
        usually set to a small positive value.

    kappa : float, optional (default: 0)
        A secondary scaling parameter which is usually set to 0.

    Returns
    -------
    sigma_points : array, shape (2 * n_features + 1, n_features)
        Query points that have to be transformed to estimate the resulting
        MVN.
    &#34;&#34;&#34;
    self._check_initialized()

    n_features = len(self.mean)
    lmbda = alpha ** 2 * (n_features + kappa) - n_features
    offset = self._sqrt_cov((n_features + lmbda) * self.covariance)

    points = np.empty(((2 * n_features + 1), n_features))
    points[0, :] = self.mean
    for i in range(n_features):
        points[1 + i, :] = self.mean + offset[i]
        points[1 + n_features + i:, :] = self.mean - offset[i]
    return points</code></pre>
</details>
<div class="desc"><p>Compute sigma points for unscented transform.</p>
<p>The unscented transform allows us to estimate the resulting MVN from
applying a nonlinear transformation :math:<code>f</code> to this MVN. In order to
do this, you have to transform the sigma points obtained from this
function with :math:<code>f</code> and then create the new MVN with
:func:<code><a title="gmr.MVN.estimate_from_sigma_points" href="#gmr.MVN.estimate_from_sigma_points">MVN.estimate_from_sigma_points()</a></code>. The unscented transform is most
commonly used in the Unscented Kalman Filter (UKF).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code>, optional <code>(default: 1e-3)</code></dt>
<dd>Determines the spread of the sigma points around the mean and is
usually set to a small positive value.</dd>
<dt><strong><code>kappa</code></strong> :&ensp;<code>float</code>, optional <code>(default: 0)</code></dt>
<dd>A secondary scaling parameter which is usually set to 0.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>sigma_points</code></strong> :&ensp;<code>array, shape (2 * n_features + 1, n_features)</code></dt>
<dd>Query points that have to be transformed to estimate the resulting
MVN.</dd>
</dl></div>
</dd>
<dt id="gmr.MVN.squared_mahalanobis_distance"><code class="name flex">
<span>def <span class="ident">squared_mahalanobis_distance</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def squared_mahalanobis_distance(self, x):
    &#34;&#34;&#34;Squared Mahalanobis distance between point and this MVN.

    Parameters
    ----------
    x : array, shape (n_features,)

    Returns
    -------
    d : float
        Squared Mahalanobis distance
    &#34;&#34;&#34;
    self._check_initialized()
    return mahalanobis(x, self.mean, np.linalg.inv(self.covariance)) ** 2</code></pre>
</details>
<div class="desc"><p>Squared Mahalanobis distance between point and this MVN.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>array, shape (n_features,)</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>d</code></strong> :&ensp;<code>float</code></dt>
<dd>Squared Mahalanobis distance</dd>
</dl></div>
</dd>
<dt id="gmr.MVN.to_ellipse"><code class="name flex">
<span>def <span class="ident">to_ellipse</span></span>(<span>self, factor=1.0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_ellipse(self, factor=1.0):
    &#34;&#34;&#34;Compute error ellipse.

    An error ellipse shows equiprobable points.

    Parameters
    ----------
    factor : float
        One means standard deviation.

    Returns
    -------
    angle : float
        Rotation angle of the ellipse.

    width : float
        Width of the ellipse (semi axis, not diameter).

    height : float
        Height of the ellipse (semi axis, not diameter).
    &#34;&#34;&#34;
    self._check_initialized()
    vals, vecs = sp.linalg.eigh(self.covariance)
    order = vals.argsort()[::-1]
    vals, vecs = vals[order], vecs[:, order]
    angle = np.arctan2(*vecs[:, 0][::-1])
    width, height = factor * np.sqrt(vals)
    return angle, width, height</code></pre>
</details>
<div class="desc"><p>Compute error ellipse.</p>
<p>An error ellipse shows equiprobable points.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>factor</code></strong> :&ensp;<code>float</code></dt>
<dd>One means standard deviation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>angle</code></strong> :&ensp;<code>float</code></dt>
<dd>Rotation angle of the ellipse.</dd>
<dt><strong><code>width</code></strong> :&ensp;<code>float</code></dt>
<dd>Width of the ellipse (semi axis, not diameter).</dd>
<dt><strong><code>height</code></strong> :&ensp;<code>float</code></dt>
<dd>Height of the ellipse (semi axis, not diameter).</dd>
</dl></div>
</dd>
<dt id="gmr.MVN.to_norm_factor_and_exponents"><code class="name flex">
<span>def <span class="ident">to_norm_factor_and_exponents</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_norm_factor_and_exponents(self, X):
    &#34;&#34;&#34;Compute normalization factor and exponents of Gaussian.

    These values can be used to compute the probability density function
    of this Gaussian: p(x) = norm_factor * np.exp(exponents).

    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        Data.

    Returns
    -------
    norm_factor : float
        Normalization factor: constant term outside of exponential
        function in probability density function of this Gaussian.

    exponents : array, shape (n_samples,)
        Exponents to compute probability density function.
    &#34;&#34;&#34;
    self._check_initialized()

    X = np.atleast_2d(X)
    n_features = X.shape[1]

    try:
        L = sp.linalg.cholesky(self.covariance, lower=True)
    except np.linalg.LinAlgError:
        # Degenerated covariance, try to add regularization
        L = sp.linalg.cholesky(
            self.covariance + 1e-3 * np.eye(n_features), lower=True)

    X_minus_mean = X - self.mean

    if self.norm is None:
        # Suppress a determinant of 0 to avoid numerical problems
        L_det = max(sp.linalg.det(L), np.finfo(L.dtype).eps)
        self.norm = (2.0 * np.pi) ** (-0.5 * n_features) / L_det

    # Solve L x = (X - mean)^T for x with triangular L
    # (LL^T = Sigma), that is, x = L^T^-1 (X - mean)^T.
    # We can avoid covariance inversion when computing
    # (X - mean) Sigma^-1 (X - mean)^T  with this trick,
    # since Sigma^-1 = L^T^-1 L^-1.
    X_normalized = sp.linalg.solve_triangular(
        L, X_minus_mean.T, lower=True).T

    exponent = -0.5 * np.sum(X_normalized ** 2, axis=1)

    return self.norm, exponent</code></pre>
</details>
<div class="desc"><p>Compute normalization factor and exponents of Gaussian.</p>
<p>These values can be used to compute the probability density function
of this Gaussian: p(x) = norm_factor * np.exp(exponents).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like, shape (n_samples, n_features)</code></dt>
<dd>Data.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>norm_factor</code></strong> :&ensp;<code>float</code></dt>
<dd>Normalization factor: constant term outside of exponential
function in probability density function of this Gaussian.</dd>
<dt><strong><code>exponents</code></strong> :&ensp;<code>array, shape (n_samples,)</code></dt>
<dd>Exponents to compute probability density function.</dd>
</dl></div>
</dd>
<dt id="gmr.MVN.to_probability_density"><code class="name flex">
<span>def <span class="ident">to_probability_density</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_probability_density(self, X):
    &#34;&#34;&#34;Compute probability density.

    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        Data.

    Returns
    -------
    p : array, shape (n_samples,)
        Probability densities of data.
    &#34;&#34;&#34;
    norm_factor, exponents = self.to_norm_factor_and_exponents(X)
    return norm_factor * np.exp(exponents)</code></pre>
</details>
<div class="desc"><p>Compute probability density.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like, shape (n_samples, n_features)</code></dt>
<dd>Data.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>p</code></strong> :&ensp;<code>array, shape (n_samples,)</code></dt>
<dd>Probability densities of data.</dd>
</dl></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul>
<li><a href="#gmr">gmr</a></li>
</ul>
</div>
<ul id="index">
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="gmr.gmm" href="gmm.html">gmr.gmm</a></code></li>
<li><code><a title="gmr.mvn" href="mvn.html">gmr.mvn</a></code></li>
<li><code><a title="gmr.sklearn" href="sklearn.html">gmr.sklearn</a></code></li>
<li><code><a title="gmr.tests" href="tests/index.html">gmr.tests</a></code></li>
<li><code><a title="gmr.utils" href="utils.html">gmr.utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="gmr.covariance_initialization" href="#gmr.covariance_initialization">covariance_initialization</a></code></li>
<li><code><a title="gmr.kmeansplusplus_initialization" href="#gmr.kmeansplusplus_initialization">kmeansplusplus_initialization</a></code></li>
<li><code><a title="gmr.plot_error_ellipse" href="#gmr.plot_error_ellipse">plot_error_ellipse</a></code></li>
<li><code><a title="gmr.plot_error_ellipses" href="#gmr.plot_error_ellipses">plot_error_ellipses</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="gmr.GMM" href="#gmr.GMM">GMM</a></code></h4>
<ul class="">
<li><code><a title="gmr.GMM.apply_oracle_approximating_shrinkage" href="#gmr.GMM.apply_oracle_approximating_shrinkage">apply_oracle_approximating_shrinkage</a></code></li>
<li><code><a title="gmr.GMM.condition" href="#gmr.GMM.condition">condition</a></code></li>
<li><code><a title="gmr.GMM.extract_mvn" href="#gmr.GMM.extract_mvn">extract_mvn</a></code></li>
<li><code><a title="gmr.GMM.from_samples" href="#gmr.GMM.from_samples">from_samples</a></code></li>
<li><code><a title="gmr.GMM.is_in_confidence_region" href="#gmr.GMM.is_in_confidence_region">is_in_confidence_region</a></code></li>
<li><code><a title="gmr.GMM.predict" href="#gmr.GMM.predict">predict</a></code></li>
<li><code><a title="gmr.GMM.sample" href="#gmr.GMM.sample">sample</a></code></li>
<li><code><a title="gmr.GMM.sample_confidence_region" href="#gmr.GMM.sample_confidence_region">sample_confidence_region</a></code></li>
<li><code><a title="gmr.GMM.to_ellipses" href="#gmr.GMM.to_ellipses">to_ellipses</a></code></li>
<li><code><a title="gmr.GMM.to_mvn" href="#gmr.GMM.to_mvn">to_mvn</a></code></li>
<li><code><a title="gmr.GMM.to_probability_density" href="#gmr.GMM.to_probability_density">to_probability_density</a></code></li>
<li><code><a title="gmr.GMM.to_responsibilities" href="#gmr.GMM.to_responsibilities">to_responsibilities</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="gmr.MVN" href="#gmr.MVN">MVN</a></code></h4>
<ul class="">
<li><code><a title="gmr.MVN.condition" href="#gmr.MVN.condition">condition</a></code></li>
<li><code><a title="gmr.MVN.estimate_from_sigma_points" href="#gmr.MVN.estimate_from_sigma_points">estimate_from_sigma_points</a></code></li>
<li><code><a title="gmr.MVN.from_samples" href="#gmr.MVN.from_samples">from_samples</a></code></li>
<li><code><a title="gmr.MVN.is_in_confidence_region" href="#gmr.MVN.is_in_confidence_region">is_in_confidence_region</a></code></li>
<li><code><a title="gmr.MVN.marginalize" href="#gmr.MVN.marginalize">marginalize</a></code></li>
<li><code><a title="gmr.MVN.predict" href="#gmr.MVN.predict">predict</a></code></li>
<li><code><a title="gmr.MVN.sample" href="#gmr.MVN.sample">sample</a></code></li>
<li><code><a title="gmr.MVN.sample_confidence_region" href="#gmr.MVN.sample_confidence_region">sample_confidence_region</a></code></li>
<li><code><a title="gmr.MVN.sigma_points" href="#gmr.MVN.sigma_points">sigma_points</a></code></li>
<li><code><a title="gmr.MVN.squared_mahalanobis_distance" href="#gmr.MVN.squared_mahalanobis_distance">squared_mahalanobis_distance</a></code></li>
<li><code><a title="gmr.MVN.to_ellipse" href="#gmr.MVN.to_ellipse">to_ellipse</a></code></li>
<li><code><a title="gmr.MVN.to_norm_factor_and_exponents" href="#gmr.MVN.to_norm_factor_and_exponents">to_norm_factor_and_exponents</a></code></li>
<li><code><a title="gmr.MVN.to_probability_density" href="#gmr.MVN.to_probability_density">to_probability_density</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
